# MicrosserviÃ§o RAG com Agente LangGraph

Este projeto implementa um sistema de Pergunta-Resposta (Question-Answering) baseado em RAG (Retrieval-Augmented Generation), orquestrado por um agente inteligente construÃ­do com LangGraph. A soluÃ§Ã£o Ã© conteinerizada com Docker e projetada para ser modular, robusta e avaliÃ¡vel.

## SumÃ¡rio
- [Arquitetura](#arquitetura)
- [Como Executar (Docker)](#como-executar-docker)
- [Endpoints da API](#endpoints-da-api)
- [AvaliaÃ§Ã£o do Sistema](#avaliaÃ§Ã£o-do-sistema)
- [Executando os Testes](#executando-os-testes)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [VariÃ¡veis de Ambiente](#variÃ¡veis-de-ambiente)
- [Status Atual e PrÃ³ximos Passos](#status-atual-e-prÃ³ximos-passos)

---

## Arquitetura

O sistema combina duas lÃ³gicas principais: um pipeline de RAG direto e um fluxo de agente mais sofisticado que o orquestra.

### 1. Fluxo do Agente (LangGraph)

O agente decide a melhor forma de responder a uma pergunta, podendo pedir mais informaÃ§Ãµes ou acionar o pipeline de RAG.

```mermaid
flowchart TD
    A[Pergunta do UsuÃ¡rio] --> B{NÃ³ de Triagem};
    B -- "Precisa de mais infos" --> C[NÃ³ de Pedir InformaÃ§Ã£o];
    B -- "Pode ser respondida" --> D{NÃ³ Auto-Resolver};
    C --> F[Fim do Fluxo];
    D --> E{RAG foi bem-sucedido?};
    E -- Sim --> F;
    E -- NÃ£o --> C;
```

### 2. Pipeline de RAG (Retrieval-Augmented Generation)

Este Ã© o nÃºcleo da busca e geraÃ§Ã£o de respostas, acionado pelo agente.

```mermaid
flowchart TD
    Q[Pergunta AutÃ´noma] --> MQ[Multi-Query];
    MQ -->|q1..qn| FAISS[(Busca Vetorial FAISS)];
    FAISS --> RERANK[CrossEncoder (Rerank)];
    RERANK --> CTX[SeleÃ§Ã£o de Contexto];
    CTX --> LLM[LLM - GeraÃ§Ã£o da Resposta Final];
    LLM --> OUT[Markdown + CitaÃ§Ãµes];
```

---

## Como Executar (Docker)

A maneira mais simples e recomendada de executar o projeto Ã© usando o Docker Compose.

### 1. PrÃ©-requisitos
- Docker e Docker Compose instalados.
- Uma chave de API do Google Gemini (obtenha em [Google AI Studio](https://aistudio.google.com/app/apikey)).

### 2. ConfiguraÃ§Ã£o

Copie o arquivo de exemplo `.env.example` para um novo arquivo chamado `.env`.

```bash
cp .env.example .env
```

Abra o arquivo `.env` e **insira sua chave de API do Google** na variÃ¡vel `GOOGLE_API_KEY`.

### 3. ExecuÃ§Ã£o

Escolha o ambiente (CPU ou GPU) e execute o comando correspondente na raiz do projeto.

**Para ambiente com CPU:**
```bash
docker-compose -f docker-compose.cpu.yml up --build
```

**Para ambiente com GPU (requer NVIDIA Container Toolkit):**
```bash
docker-compose -f docker-compose.gpu.yml up --build
```

O primeiro build pode demorar alguns minutos. ApÃ³s a inicializaÃ§Ã£o, os seguintes serviÃ§os estarÃ£o disponÃ­veis:
- **API do RAG:** `http://localhost:5000`
- **Interface Web (UI):** `http://localhost:8080`
- **Banco de Dados (Postgres):** `localhost:5432`

> **Dica:** a interface web (porta 8080) encaminha qualquer chamada para `/api/...` ao mesmo serviÃ§o Flask na porta 5000. Exemplos: `http://localhost:5000/query` (direto) e `http://localhost:8080/api/query` (via UI).

---

## Endpoints da API

### Endpoint Principal do Agente

- **URL:** `POST /agent/ask`
- **Alias:** `POST /api/ask` (exposto pelo proxy da UI em `http://localhost:8080`)
- **DescriÃ§Ã£o:** Processa uma pergunta usando o fluxo completo do agente LangGraph. Suporta histÃ³rico de conversa e aceita overrides via payload.
- **Payload (JSON):**
  ```json
  {
    "question": "Qual Ã© o e-mail do departamento de biologia?",
    "messages": [
      {"role": "user", "content": "Qual o contato do depto de bio?"},
      {"role": "assistant", "content": "NÃ£o encontrei um departamento com esse nome. Poderia especificar o nome completo?"}
    ],
    "max_refine_attempts": 1
  }
  ```
  - `max_refine_attempts` (opcional) limita quantas reformulaÃ§Ãµes automÃ¡ticas o agente pode fazer na chamada (padrÃ£o: `AGENT_REFINE_MAX_ATTEMPTS`).
  - O corpo da resposta inclui `meta.query_hash`, `meta.refine_history`, `meta.refine_prompt_hashes` e `meta.confidence`, Ãºteis para correlacionar com os registros e mÃ©tricas.

### Endpoint Legado (RAG Direto)

- **URL:** `POST /query`
- **Alias:** `POST /api/query` (quando acessado via Nginx/porta 8080)
- **DescriÃ§Ã£o:** Processa uma pergunta usando apenas o pipeline de RAG direto, sem a camada do agente.

Ambos os endpoints retornam o campo opcional `"needs_clarification"` quando o sistema encontra mÃºltiplos candidatos (ex.: pessoas com o mesmo nome). Isso gera uma resposta enumerando as opÃ§Ãµes disponÃ­veis e incrementa o contador `queries_ambiguous` em `/metrics`.

---

## Cache de Respostas (Redis)

- O `docker-compose.*` agora sobe um serviÃ§o Redis (`ai_redis`) que atende ao cache de respostas.
- Configure `REDIS_URL` (padrÃ£o `redis://ai_redis:6379/0`) e `CACHE_TTL_SECONDS` (12 horas) para controlar a camada de cache.
- O ETL invalida o cache automaticamente ao final de rebuilds e atualizaÃ§Ãµes incrementais, e a variÃ¡vel `INDEX_VERSION` faz parte da assinatura das chaves.
- `/metrics` passou a expor `cache_hits_total` e `cache_misses_total`, permitindo acompanhar a eficiÃªncia do cache.
- Para garantir o funcionamento, hÃ¡ testes dedicados: `python -m pytest tests/test_api_cache.py`.
- Consulte `docs/METRICS.md` para exemplos de consulta e alertas Prometheus/Grafana.

---

## Auto-refine do Agente LangGraph

- Quando o RAG entrega baixa confianÃ§a, o agente tenta atÃ© duas reformulaÃ§Ãµes automÃ¡ticas antes de pedir mais contexto ao usuÃ¡rio.
- As novas consultas sÃ£o geradas via LLM (`refine_query_prompt.txt`) e reutilizam o pipeline completo de RAG, registrando o histÃ³rico (e hashes) em `meta.refine_history`/`meta.refine_prompt_hashes`.
- Novas mÃ©tricas expostas em `/metrics`: `agent_refine_attempts_total`, `agent_refine_success_total`, `agent_refine_exhausted_total` e `agent_low_confidence_total`.
- Ajuste a estratÃ©gia com `AGENT_REFINE_ENABLED`, `AGENT_REFINE_MAX_ATTEMPTS`, `AGENT_REFINE_CONFIDENCE`, o limiar de confianÃ§a por rota (`CONFIDENCE_MIN_AGENT`) e, via payload, `max_refine_attempts`.
- A suÃ­te `python -m pytest tests/test_agent_workflow.py` valida fluxos de sucesso e fallback.

## AvaliaÃ§Ã£o do Sistema

O projeto inclui um script de avaliaÃ§Ã£o de ponta a ponta que usa `ragas` e `langchain-google-genai` para calcular mÃ©tricas automÃ¡ticas.

1.  Garanta que as dependÃªncias estejam instaladas (`pip install -r requirements-cpu.txt`) e que a variÃ¡vel `GOOGLE_API_KEY` esteja configurada no ambiente.
2.  Inicie a API localmente (Docker Compose ou ambiente manual).
3.  Execute o script `eval_rag.py`, informando o dataset via argumento posicional ou pelo flag `--dataset`.

```bash
# Exemplo de execuÃ§Ã£o com saÃ­da em reports/
python eval_rag.py --dataset tests/eval_sample.csv --out reports/
```

Use `--agent-endpoint` e `--legacy-endpoint` para apontar para URLs especÃ­ficas quando necessÃ¡rio.

O relatÃ³rio consolida mÃ©tricas de **RecuperaÃ§Ã£o** (Recall, MRR, nDCG). Quando a chave `GOOGLE_API_KEY` estÃ¡ definida, o script tambÃ©m produz as mÃ©tricas de **GeraÃ§Ã£o** (Faithfulness e Answer Relevancy) via RAGAs.
> **Checklist rÃ¡pido:** o script agora emite mensagens explÃ­citas quando as dependÃªncias do RAGAs ou a `GOOGLE_API_KEY` estÃ£o ausentes. O teste `python -m pytest tests/test_eval_rag.py` cobre esses cenÃ¡rios e garante proteÃ§Ã£o regressiva.

## Executando os Testes

O projeto utiliza `pytest` para testes automatizados. Para executar a suÃ­te de testes:

1.  Instale as dependÃªncias correspondentes ao seu ambiente (fora do Docker elas precisam incluir FAISS e Torch):
    ```bash
    pip install -r requirements-cpu.txt  # ou requirements-gpu.txt se estiver com CUDA
    ```
2.  Execute o pytest na raiz do projeto:
    ```bash
    pytest -v
    ```

---

## Estrutura do Projeto

```
. C:/Temp/Workspace/rag-microservice
â”œâ”€â”€ ğŸ“„ .env.example        # Exemplo de arquivo de configuraÃ§Ã£o
â”œâ”€â”€ ğŸ“„ api.py              # Servidor Flask, expÃµe os endpoints da API
â”œâ”€â”€ ğŸ“„ agent_workflow.py   # Orquestra a lÃ³gica do agente com LangGraph
â”œâ”€â”€ ğŸ“„ query_handler.py    # Implementa a lÃ³gica central de RAG (busca e geraÃ§Ã£o)
â”œâ”€â”€ ğŸ“„ llm_client.py       # Cliente unificado e robusto para interagir com LLMs
â”œâ”€â”€ ğŸ“„ etl_orchestrator.py # Pipeline de ETL para construir o Ã­ndice vetorial
â”œâ”€â”€ ğŸ“„ eval_rag.py         # Script para avaliaÃ§Ã£o de ponta-a-ponta do sistema
â”œâ”€â”€ ğŸ“„ telemetry.py        # MÃ³dulo de logging de telemetria
â”œâ”€â”€ ğŸ“ loaders/           # MÃ³dulo unificado para carregar documentos de diferentes formatos
â”œâ”€â”€ ğŸ“ prompts/           # Armazena os prompts usados pelo agente e pelo RAG
â”œâ”€â”€ ğŸ“ data/              # ContÃ©m os documentos fonte para o ETL
â”œâ”€â”€ ğŸ“ config/            # Arquivos de configuraÃ§Ã£o adicionais (ex: ontologias)
â”œâ”€â”€ ğŸ“ tests/             # Testes automatizados com pytest
â”œâ”€â”€ ğŸ³ Dockerfile.cpu      # Define a imagem Docker para ambiente CPU
â”œâ”€â”€ ğŸ³ Dockerfile.gpu      # Define a imagem Docker para ambiente GPU
â”œâ”€â”€ ğŸ³ docker-compose.cpu.yml  # Stack completa otimizada para CPU
â””â”€â”€ ğŸ³ docker-compose.gpu.yml  # Stack completa com suporte a GPU
```

---

## VariÃ¡veis de Ambiente

As principais variÃ¡veis de ambiente para configurar o comportamento do sistema estÃ£o no arquivo `.env`. Consulte o `.env.example` para uma lista completa e descriÃ§Ãµes detalhadas. Destaques:

- **LLM**: `GOOGLE_API_KEY` e `GOOGLE_MODEL` (ex.: `models/gemini-2.5-flash-lite`).
- **Embeddings/FAISS**: `EMBEDDINGS_MODEL` (ETL e API precisam usar o mesmo valor) e `FAISS_STORE_DIR`.
- **Reranker**: `RERANKER_PRESET` (`off | fast | balanced | full`) e `RERANKER_ENABLED=true`. O preset `balanced` usa `jinaai/jina-reranker-v1-base-multilingual` (boa qualidade no CPU). Ajuste `RERANKER_CANDIDATES`, `RERANKER_TOP_K`, `RERANKER_MAX_LEN` conforme latÃªncia desejada.
- **Busca hÃ­brida**: `HYBRID_ENABLED` (default `true`), `LEXICAL_THRESHOLD` (default `90`), `DEPT_BONUS`, `MAX_PER_SOURCE`.
- **Multi-query e confianÃ§a**: `MQ_ENABLED`, `MQ_VARIANTS`, `CONFIDENCE_MIN` (com overrides opcionais `CONFIDENCE_MIN_QUERY`/`CONFIDENCE_MIN_AGENT`) e `REQUIRE_CONTEXT`.
- **Formato da resposta**: `STRUCTURED_ANSWER` (markdown com resumo/fontes) e `MAX_SOURCES`.
- **Agente (auto-refine)**: `AGENT_REFINE_ENABLED`, `AGENT_REFINE_MAX_ATTEMPTS`, `AGENT_REFINE_CONFIDENCE`.

Caso queira forÃ§ar uma rota especÃ­fica para debug, use `ROUTE_FORCE=lexical|vector`.

---

## Status Atual e PrÃ³ximos Passos

### VerificaÃ§Ã£o Recente do Reranker

Em testes recentes, foi verificado que o componente de reranqueamento estÃ¡ ativo e funcional (`"enabled": true`). Ele reordena os *chunks* recuperados pela busca vetorial, aplicando uma lÃ³gica de relevÃ¢ncia mais refinada.

**ConclusÃ£o:** O reranker funciona como esperado. No entanto, a resposta final gerada a partir do *chunk* melhor classificado apresentou um score de confianÃ§a muito baixo (ex: 0.0036). Isso indica que, embora o reordenamento tÃ©cnico funcione, a relevÃ¢ncia do conteÃºdo recuperado ainda nÃ£o Ã© Ã³tima para responder a certas perguntas, sendo um ponto-chave para as prÃ³ximas melhorias.

### SugestÃµes de Melhoria

Para aumentar a relevÃ¢ncia e a confianÃ§a das respostas, as seguintes aÃ§Ãµes sÃ£o recomendadas:

1.  **ReforÃ§o Lexical e Boosts**:
    *   Ajustar as expansÃµes de multi-query em `CUSTOM_MQ_EXPANSIONS` para gerar variaÃ§Ãµes mais ricas e direcionadas (ex: incluir termos como "e-mail stpg reserva").
    *   Analisar as `mq_variants` (com `debug=true`) para validar a eficÃ¡cia das novas expansÃµes.

2.  **Afinar Termos da Ontologia**:
    *   Em `config/ontology/terms.yml`, aumentar os `boosts` de termos importantes e adicionar mais sinÃ´nimos (`aliases`) para conceitos como "reserva", "impressÃ£o", "LEPEC", etc.

3.  **ReavaliaÃ§Ã£o ContÃ­nua**:
    *   ApÃ³s cada ajuste, rodar o script de avaliaÃ§Ã£o para medir o impacto de forma objetiva:
        ```bash
        python eval_rag.py --compare --label "nome-do-experimento"
        ```

Para um roteiro mais detalhado de melhorias planejadas, consulte o documento [melhorias.md](docs/melhorias.md).



## DocumentaÃ§Ã£o adicional

Toda a documentaÃ§Ã£o complementar foi movida para a pasta docs/:

- [VisÃ£o de arquitetura](docs/ARCHITECTURE.md)
- [Contexto operacional](docs/contexto.md)
- [Plano de melhorias e backlog](docs/melhorias.md)
- [Telemetria e observabilidade](docs/TELEMETRY.md)
- [PublicaÃ§Ã£o de imagens Docker](docs/publish_images.md)

### Imagens prÃ©-construÃ­das (opcional)

Resumo rÃ¡pido:
1. FaÃ§a login no registry (ex.: docker login, docker login ghcr.io).
2. Rode ./scripts/build_and_publish_images.sh --prefix SEU_PREFIXO --tag SUA_TAG --push.
3. Configure as variÃ¡veis RAG_IMAGE_PREFIX e RAG_IMAGE_TAG (via shell ou arquivo .env).
4. Depois execute docker-compose pull + docker-compose up para usar as imagens publicadas.

Para o passo a passo completo consulte [docs/publish_images.md](docs/publish_images.md).
ObservaÃ§Ã£o: o script gera imagens para ai_projeto_api e ai_etl. A UI continua usando nginx:1.27-alpine e nÃ£o precisa de push.




