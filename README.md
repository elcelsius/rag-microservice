# MicrosserviÃ§o RAG com Agente LangGraph

Este projeto implementa um sistema de Pergunta-Resposta (Question-Answering) baseado em RAG (Retrieval-Augmented Generation), orquestrado por um agente inteligente construÃ­do com LangGraph. A soluÃ§Ã£o Ã© conteinerizada com Docker e projetada para ser modular, robusta e avaliÃ¡vel.

## SumÃ¡rio
- [Arquitetura](#arquitetura)
- [Como Executar (Docker)](#como-executar-docker)
- [Endpoints da API](#endpoints-da-api)
- [AvaliaÃ§Ã£o do Sistema](#avaliaÃ§Ã£o-do-sistema)
- [Executando os Testes](#executando-os-testes)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [VariÃ¡veis de Ambiente](#variÃ¡veis-de-ambiente)
- [Status Atual e PrÃ³ximos Passos](#status-atual-e-prÃ³ximos-passos)

---

## Arquitetura

O sistema combina duas lÃ³gicas principais: um pipeline de RAG direto e um fluxo de agente mais sofisticado que o orquestra.

### 1. Fluxo do Agente (LangGraph)

O agente decide a melhor forma de responder a uma pergunta, podendo pedir mais informaÃ§Ãµes ou acionar o pipeline de RAG.

```mermaid
flowchart TD
    A[Pergunta do UsuÃ¡rio] --> B{NÃ³ de Triagem};
    B -- "Precisa de mais infos" --> C[NÃ³ de Pedir InformaÃ§Ã£o];
    B -- "Pode ser respondida" --> D{NÃ³ Auto-Resolver};
    C --> F[Fim do Fluxo];
    D --> E{RAG foi bem-sucedido?};
    E -- Sim --> F;
    E -- NÃ£o --> C;
```

### 2. Pipeline de RAG (Retrieval-Augmented Generation)

Este Ã© o nÃºcleo da busca e geraÃ§Ã£o de respostas, acionado pelo agente.

```mermaid
flowchart TD
    Q[Pergunta AutÃ´noma] --> MQ[Multi-Query];
    MQ -->|q1..qn| FAISS[(Busca Vetorial FAISS)];
    FAISS --> RERANK[CrossEncoder (Rerank)];
    RERANK --> CTX[SeleÃ§Ã£o de Contexto];
    CTX --> LLM[LLM - GeraÃ§Ã£o da Resposta Final];
    LLM --> OUT[Markdown + CitaÃ§Ãµes];
```

---

## Como Executar (Docker)

A maneira mais simples e recomendada de executar o projeto Ã© usando o Docker Compose.

### 1. PrÃ©-requisitos
- Docker e Docker Compose instalados.
- Uma chave de API do Google Gemini (obtenha em [Google AI Studio](https://aistudio.google.com/app/apikey)).

### 2. ConfiguraÃ§Ã£o

Copie o arquivo de exemplo `.env.example` para um novo arquivo chamado `.env`.

```bash
cp .env.example .env
```

Abra o arquivo `.env` e **insira sua chave de API do Google** na variÃ¡vel `GOOGLE_API_KEY`.

### 3. ExecuÃ§Ã£o

Escolha o ambiente (CPU ou GPU) e execute o comando correspondente na raiz do projeto.

**Para ambiente com CPU:**
```bash
docker-compose -f docker-compose.cpu.yml up --build
```

**Para ambiente com GPU (requer NVIDIA Container Toolkit):**
```bash
docker-compose -f docker-compose.gpu.yml up --build
```

O primeiro build pode demorar alguns minutos. ApÃ³s a inicializaÃ§Ã£o, os seguintes serviÃ§os estarÃ£o disponÃ­veis:
- **API do RAG:** `http://localhost:5000`
- **Interface Web (UI):** `http://localhost:8080`
- **Banco de Dados (Postgres):** `localhost:5432`

---

## Endpoints da API

### Endpoint Principal do Agente

- **URL:** `POST /agent/ask`
- **DescriÃ§Ã£o:** Processa uma pergunta usando o fluxo completo do agente LangGraph. Suporta histÃ³rico de conversa.
- **Payload (JSON):**
  ```json
  {
    "question": "Qual Ã© o e-mail do departamento de biologia?",
    "messages": [
      {"role": "user", "content": "Qual o contato do depto de bio?"},
      {"role": "assistant", "content": "NÃ£o encontrei um departamento com esse nome. Poderia especificar o nome completo?"}
    ]
  }
  ```

### Endpoint Legado (RAG Direto)

- **URL:** `POST /query`
- **DescriÃ§Ã£o:** Processa uma pergunta usando apenas o pipeline de RAG direto, sem a camada do agente.

---

## AvaliaÃ§Ã£o do Sistema

O projeto inclui um script de avaliaÃ§Ã£o de ponta a ponta que utiliza a biblioteca `ragas`.

1.  **Garanta que a API esteja em execuÃ§Ã£o.**
2.  Execute o script `eval_rag.py`, passando o caminho para um arquivo CSV com os dados de teste.

```bash
# Exemplo de execuÃ§Ã£o
python eval_rag.py tests/eval_sample.csv
```

O script irÃ¡ calcular e exibir mÃ©tricas de **RecuperaÃ§Ã£o** (Recall, MRR, nDCG) e de **GeraÃ§Ã£o** (Faithfulness, Answer Relevancy).

---

## Executando os Testes

O projeto utiliza `pytest` para testes automatizados. Para executar a suÃ­te de testes:

1.  Instale as dependÃªncias correspondentes ao seu ambiente (fora do Docker elas precisam incluir FAISS e Torch):
    ```bash
    pip install -r requirements-cpu.txt  # ou requirements-gpu.txt se estiver com CUDA
    ```
2.  Execute o pytest na raiz do projeto:
    ```bash
    pytest -v
    ```

---

## Estrutura do Projeto

```
. C:/Temp/Workspace/rag-microservice
â”œâ”€â”€ ğŸ“„ .env.example        # Exemplo de arquivo de configuraÃ§Ã£o
â”œâ”€â”€ ğŸ“„ api.py              # Servidor Flask, expÃµe os endpoints da API
â”œâ”€â”€ ğŸ“„ agent_workflow.py   # Orquestra a lÃ³gica do agente com LangGraph
â”œâ”€â”€ ğŸ“„ query_handler.py    # Implementa a lÃ³gica central de RAG (busca e geraÃ§Ã£o)
â”œâ”€â”€ ğŸ“„ llm_client.py       # Cliente unificado e robusto para interagir com LLMs
â”œâ”€â”€ ğŸ“„ etl_orchestrator.py # Pipeline de ETL para construir o Ã­ndice vetorial
â”œâ”€â”€ ğŸ“„ eval_rag.py         # Script para avaliaÃ§Ã£o de ponta-a-ponta do sistema
â”œâ”€â”€ ğŸ“„ telemetry.py        # MÃ³dulo de logging de telemetria
â”œâ”€â”€ ğŸ“ loaders/           # MÃ³dulo unificado para carregar documentos de diferentes formatos
â”œâ”€â”€ ğŸ“ prompts/           # Armazena os prompts usados pelo agente e pelo RAG
â”œâ”€â”€ ğŸ“ data/              # ContÃ©m os documentos fonte para o ETL
â”œâ”€â”€ ğŸ“ config/            # Arquivos de configuraÃ§Ã£o adicionais (ex: ontologias)
â”œâ”€â”€ ğŸ“ tests/             # Testes automatizados com pytest
â”œâ”€â”€ ğŸ³ Dockerfile.cpu      # Define a imagem Docker para ambiente CPU
â”œâ”€â”€ ğŸ³ Dockerfile.gpu      # Define a imagem Docker para ambiente GPU
â”œâ”€â”€ ğŸ³ docker-compose.cpu.yml  # Stack completa otimizada para CPU
â””â”€â”€ ğŸ³ docker-compose.gpu.yml  # Stack completa com suporte a GPU
```

---

## VariÃ¡veis de Ambiente

As principais variÃ¡veis de ambiente para configurar o comportamento do sistema estÃ£o no arquivo `.env`. Consulte o `.env.example` para uma lista completa e descriÃ§Ãµes detalhadas. Destaques:

- **LLM**: `GOOGLE_API_KEY` e `GOOGLE_MODEL` (ex.: `models/gemini-2.5-flash-lite`).
- **Embeddings/FAISS**: `EMBEDDINGS_MODEL` (ETL e API precisam usar o mesmo valor) e `FAISS_STORE_DIR`.
- **Reranker**: `RERANKER_PRESET` (`off | fast | balanced | full`) e `RERANKER_ENABLED=true`. O preset `balanced` usa `jinaai/jina-reranker-v1-base-multilingual` (boa qualidade no CPU). Ajuste `RERANKER_CANDIDATES`, `RERANKER_TOP_K`, `RERANKER_MAX_LEN` conforme latÃªncia desejada.
- **Busca hÃ­brida**: `HYBRID_ENABLED` (default `true`), `LEXICAL_THRESHOLD` (default `90`), `DEPT_BONUS`, `MAX_PER_SOURCE`.
- **Multi-query e confianÃ§a**: `MQ_ENABLED`, `MQ_VARIANTS`, `CONFIDENCE_MIN` e `REQUIRE_CONTEXT`.
- **Formato da resposta**: `STRUCTURED_ANSWER` (markdown com resumo/fontes) e `MAX_SOURCES`.

Caso queira forÃ§ar uma rota especÃ­fica para debug, use `ROUTE_FORCE=lexical|vector`.

---

## Status Atual e PrÃ³ximos Passos

### VerificaÃ§Ã£o Recente do Reranker

Em testes recentes, foi verificado que o componente de reranqueamento estÃ¡ ativo e funcional (`"enabled": true`). Ele reordena os *chunks* recuperados pela busca vetorial, aplicando uma lÃ³gica de relevÃ¢ncia mais refinada.

**ConclusÃ£o:** O reranker funciona como esperado. No entanto, a resposta final gerada a partir do *chunk* melhor classificado apresentou um score de confianÃ§a muito baixo (ex: 0.0036). Isso indica que, embora o reordenamento tÃ©cnico funcione, a relevÃ¢ncia do conteÃºdo recuperado ainda nÃ£o Ã© Ã³tima para responder a certas perguntas, sendo um ponto-chave para as prÃ³ximas melhorias.

### SugestÃµes de Melhoria

Para aumentar a relevÃ¢ncia e a confianÃ§a das respostas, as seguintes aÃ§Ãµes sÃ£o recomendadas:

1.  **ReforÃ§o Lexical e Boosts**:
    *   Ajustar as expansÃµes de multi-query em `CUSTOM_MQ_EXPANSIONS` para gerar variaÃ§Ãµes mais ricas e direcionadas (ex: incluir termos como "e-mail stpg reserva").
    *   Analisar as `mq_variants` (com `debug=true`) para validar a eficÃ¡cia das novas expansÃµes.

2.  **Afinar Termos da Ontologia**:
    *   Em `config/ontology/terms.yml`, aumentar os `boosts` de termos importantes e adicionar mais sinÃ´nimos (`aliases`) para conceitos como "reserva", "impressÃ£o", "LEPEC", etc.

3.  **ReavaliaÃ§Ã£o ContÃ­nua**:
    *   ApÃ³s cada ajuste, rodar o script de avaliaÃ§Ã£o para medir o impacto de forma objetiva:
        ```bash
        python eval_rag.py --compare --label "nome-do-experimento"
        ```

Para um roteiro mais detalhado de melhorias planejadas, consulte o documento [melhorias.md](./melhorias.md).