# ü§ñ AI Copilot - Servi√ßo de ETL e RAG Gen√©rico

Este projeto implementa um pipeline completo de **Retrieval-Augmented Generation (RAG)**, projetado para servir como o n√∫cleo de um copiloto de IA para sistemas web complexos.

O objetivo √© **ler, processar e indexar** uma base de conhecimento privada (documenta√ß√£o, c√≥digo-fonte, etc.) e fornecer uma interface de consulta inteligente, capaz de responder perguntas complexas com alta precis√£o, utilizando a API do **Google Gemini**.

O sistema √© otimizado para ambientes com **GPU NVIDIA**, mas √© totalmente compat√≠vel com ambientes **apenas com CPU** atrav√©s de scripts dedicados.

---

## üìã Principais Funcionalidades
- **Pipeline de ETL Inteligente**: Suporta m√∫ltiplos formatos de arquivo e oferece dois modos de treinamento: rebuild completo ou atualiza√ß√£o incremental (apenas para arquivos novos).
- **Base de Conhecimento Vetorial**: Utiliza *sentence-transformers* para gerar embeddings e **FAISS** para busca vetorial eficiente.
- **Portabilidade CPU/GPU**: Ambiente containerizado com suporte expl√≠cito para execu√ß√£o acelerada por **CUDA** ou em modo **CPU-only**.
- **Persist√™ncia de Metadados**: Armazenamento de chunks e rastreamento de arquivos processados em **PostgreSQL**.
- **Agente de IA com LangGraph**: Um agente inteligente avalia as perguntas antes de agir, decidindo entre responder ou pedir mais informa√ß√µes.
- **Gera√ß√£o de Respostas com LLM**: Integra√ß√£o com a API do **Google Gemini**, com modelo configur√°vel via vari√°veis de ambiente.

---

## üõ†Ô∏è Stack de Tecnologias
- **Linguagem**: Python 3.11
- **Orquestra√ß√£o**: Docker & Docker Compose
- **IA & Machine Learning**:
  - LangChain, LangGraph
  - Sentence Transformers (*all-MiniLM-L6-v2*)
  - FAISS-GPU / FAISS-CPU
  - PyTorch
  - Google Generative AI (Gemma, Gemini)
- **Banco de Dados**: PostgreSQL 15
- **Ambiente Base**: Imagem NVIDIA CUDA (GPU) ou Python Slim (CPU)

---

## üöÄ Configura√ß√£o do Ambiente

### ‚úÖ Pr√©-requisitos
- Docker Desktop
- WSL2 (para usu√°rios Windows)
- **Para modo GPU**: Drivers NVIDIA com suporte a CUDA instalados no host.

### üîß Instala√ß√£o
1. Clone o reposit√≥rio:
   ```bash
   git clone [https://github.com/elcelsius/ai_etl_project.git](https://github.com/elcelsius/ai_etl_project.git)
   cd ai_etl_project
   ```
2. Configure as vari√°veis de ambiente (copie `.env.example` para `.env` e preencha sua `GOOGLE_API_KEY`).
3. Adicione seus arquivos de documenta√ß√£o na pasta `data/`.
4. D√™ permiss√£o de execu√ß√£o para os scripts:
   ```bash
   chmod +x scripts/*.sh
   ```

---

## üí° Fluxo de Trabalho (Como Usar)
Escolha o ambiente de acordo com seu hardware.

### Op√ß√£o 1: Ambiente com GPU NVIDIA (Recomendado)
Use os scripts localizados em `scripts/` com o sufixo `_gpu`.

**Para treinar a IA (ETL):**
```bash
# Rebuild completo (lento, apaga tudo e refaz)
./scripts/treinar_ia_gpu.sh

# Atualiza√ß√£o incremental (r√°pido, adiciona somente arquivos novos)
./scripts/treinar_ia_gpu.sh --update
```

**Para iniciar o site e conversar pela interface web:**
```bash
./scripts/inicia_site_gpu.sh
```

**Para conversar pelo terminal:**
```bash
./scripts/ai_etl_conv_term_gpu.sh
```

### Op√ß√£o 2: Ambiente Apenas com CPU
Use os scripts localizados em `scripts/` com o sufixo `_cpu`.

**Para treinar a IA (ETL):**
```bash
# Rebuild completo (lento, apaga tudo e refaz)
./scripts/treinar_ia_cpu.sh

# Atualiza√ß√£o incremental (r√°pido, adiciona somente arquivos novos)
./scripts/treinar_ia_cpu.sh --update
```

**Para iniciar o site e conversar pela interface web:**
```bash
./scripts/inicia_site_cpu.sh
```

**Para conversar pelo terminal:**
```bash
./scripts/ai_etl_conv_term_cpu.sh
```

---

## ‚öôÔ∏è Como o Sistema Funciona

O projeto √© dividido em tr√™s componentes principais: o pipeline de ETL, o servi√ßo de API RAG e o agente de IA.

### 1. Pipeline de ETL (Extract, Transform, Load)

Respons√°vel por processar a base de conhecimento e criar um √≠ndice vetorial para busca. Implementado em `etl_orchestrator.py`.

- **Extra√ß√£o**: Carrega documentos de diversos formatos (`.pdf`, `.docx`, `.md`, `.txt`, c√≥digo) da pasta `data/` usando loaders espec√≠ficos (definidos em `loaders/`).
- **Transforma√ß√£o**: Os documentos s√£o divididos em pequenos peda√ßos (chunks) usando `RecursiveCharacterTextSplitter` para otimizar a busca. Metadados como `source_file` s√£o associados a cada chunk.
- **Carregamento (Load)**:
    - **Embeddings**: Cada chunk √© convertido em um vetor num√©rico (embedding) usando o modelo `sentence-transformers/all-MiniLM-L6-v2`.
    - **Vector Store (FAISS)**: Os embeddings s√£o armazenados em um √≠ndice FAISS, que permite buscas de similaridade eficientes.
    - **Persist√™ncia de Metadados (PostgreSQL)**: Informa√ß√µes sobre os chunks e os arquivos processados (incluindo hashes para detec√ß√£o de modifica√ß√µes) s√£o armazenadas em um banco de dados PostgreSQL. Isso permite atualiza√ß√µes incrementais e rastreamento da base de conhecimento.

### 2. Servi√ßo de API RAG

Uma API Flask (`api.py`) que exp√µe endpoints para consultas. Ela √© respons√°vel por receber as perguntas do usu√°rio, buscar no √≠ndice vetorial e orquestrar a gera√ß√£o da resposta.

- **Health Checks e M√©tricas**: Inclui endpoints `/healthz` para verificar a prontid√£o da aplica√ß√£o (FAISS e LLM) e `/metrics` para monitorar o tempo de atividade e o n√∫mero de consultas.
- **Processamento de Consultas**: Ao receber uma pergunta, a API utiliza o modelo de embeddings e o vetorstore FAISS para encontrar os chunks de documentos mais relevantes.
- **Gera√ß√£o de Resposta**: Os chunks recuperados s√£o passados para o fun√ß√£o `answer_question` (em `query_handler.py`) que utiliza um LLM (Google Gemini) para gerar uma resposta coerente e citar as fontes.

### 3. Agente de IA (LangGraph)

Um agente inteligente (`agent_workflow.py`) constru√≠do com LangGraph que gerencia o fluxo de conversa√ß√£o.

- **Triagem**: O agente primeiro classifica a pergunta do usu√°rio (`node_triagem`) para decidir se pode ser respondida diretamente ou se requer mais informa√ß√µes.
- **Auto-Resolu√ß√£o (RAG)**: Se a pergunta for clara, o agente tenta resolv√™-la usando o pipeline RAG (`node_auto_resolver`). Se houver hist√≥rico de conversa, a pergunta √© condensada para ser aut√¥noma antes de ser enviada ao RAG.
- **Pedido de Informa√ß√µes**: Se a pergunta for amb√≠gua ou o RAG n√£o encontrar contexto suficiente, o agente formula uma pergunta de esclarecimento ao usu√°rio (`node_pedir_info`) usando o LLM.
- **Tomada de Decis√£o**: A l√≥gica condicional (`decidir_pos_triagem`, `decidir_pos_auto_resolver`) direciona o fluxo do grafo com base nos resultados da triagem e do RAG.

---

‚úçÔ∏è Autor: Celso Lisboa
üìé Reposit√≥rio: github.com/elcelsius/ai_etl_project


