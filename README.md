# MicrosserviÃ§o RAG com Agente LangGraph

Este projeto implementa um sistema de Pergunta-Resposta (Question-Answering) baseado em RAG (Retrieval-Augmented Generation), orquestrado por um agente inteligente construÃ­do com LangGraph. A soluÃ§Ã£o Ã© conteinerizada com Docker e projetada para ser modular, robusta e avaliÃ¡vel.

## SumÃ¡rio
- [Arquitetura](#arquitetura)
- [Como Executar (Docker)](#como-executar-docker)
- [Endpoints da API](#endpoints-da-api)
- [AvaliaÃ§Ã£o do Sistema](#avaliaÃ§Ã£o-do-sistema)
- [Executando os Testes](#executando-os-testes)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [VariÃ¡veis de Ambiente](#variÃ¡veis-de-ambiente)
- [Status Atual e PrÃ³ximos Passos](#status-atual-e-prÃ³ximos-passos)

---

## Arquitetura

O sistema combina duas lÃ³gicas principais: um pipeline de RAG direto e um fluxo de agente mais sofisticado que o orquestra.

### 1. Fluxo do Agente (LangGraph)

O agente decide a melhor forma de responder a uma pergunta, podendo pedir mais informaÃ§Ãµes ou acionar o pipeline de RAG.

```mermaid
flowchart TD
    A[Pergunta do UsuÃ¡rio] --> B{NÃ³ de Triagem};
    B -- "Precisa de mais infos" --> C[NÃ³ de Pedir InformaÃ§Ã£o];
    B -- "Pode ser respondida" --> D{NÃ³ Auto-Resolver};
    C --> F[Fim do Fluxo];
    D --> E{RAG foi bem-sucedido?};
    E -- Sim --> F;
    E -- NÃ£o --> C;
```

### 2. Pipeline de RAG (Retrieval-Augmented Generation)

Este Ã© o nÃºcleo da busca e geraÃ§Ã£o de respostas, acionado pelo agente.

```mermaid
flowchart TD
    Q[Pergunta AutÃ´noma] --> MQ[Multi-Query];
    MQ -->|q1..qn| FAISS[(Busca Vetorial FAISS)];
    MQ -->|q1..qn| BM25[(Busca Lexical BM25)];
    FAISS --> FUSE[MMR + RRF];
    BM25 --> FUSE;
    FUSE --> RERANK[CrossEncoder - Rerank];
    RERANK --> CTX[SeleÃ§Ã£o de Contexto];
    CTX --> LLM[LLM - GeraÃ§Ã£o da Resposta Final];
    LLM --> OUT[Markdown + CitaÃ§Ãµes];
```

O pipeline executa mÃºltiplas variaÃ§Ãµes da pergunta, consulta simultaneamente o Ã­ndice vetorial (FAISS) e a busca lexical BM25, aplica **MMR (Î»â‰ˆ0,3)** para evitar duplicidade, realiza a fusÃ£o **RRF** e, por fim, refina a ordem com o cross-encoder `cross-encoder/ms-marco-MiniLM-L-6-v2` (configurÃ¡vel no `.env`). Caso a confianÃ§a final fique abaixo do limiar configurado, o sistema tenta automaticamente uma segunda rodada de busca, expandindo a consulta com sinÃ´nimos institucionais antes de pedir esclarecimentos ao usuÃ¡rio.

---

## Como Executar (Docker)

A maneira mais simples e recomendada de executar o projeto Ã© usando o Docker Compose.

### 1. PrÃ©-requisitos
- Docker e Docker Compose instalados.
- Uma chave de API do Google Gemini (obtenha em [Google AI Studio](https://aistudio.google.com/app/apikey)).

### 2. ConfiguraÃ§Ã£o

Copie o arquivo de exemplo `.env.example` para um novo arquivo chamado `.env`.

```bash
cp .env.example .env
```

Abra o arquivo `.env` e **insira sua chave de API do Google** na variÃ¡vel `GOOGLE_API_KEY`.

### 3. ExecuÃ§Ã£o

Escolha o ambiente (CPU ou GPU) e execute o comando correspondente na raiz do projeto.

**Para ambiente com CPU:**
```bash
docker-compose -f docker-compose.cpu.yml up --build
```

**Para ambiente com GPU (requer NVIDIA Container Toolkit):**
```bash
docker-compose -f docker-compose.gpu.yml up --build
```

O primeiro build pode demorar alguns minutos. ApÃ³s a inicializaÃ§Ã£o, os seguintes serviÃ§os estarÃ£o disponÃ­veis:
- **API do RAG:** `http://localhost:5000`
- **Interface Web (UI):** `http://localhost:8080`
- **Banco de Dados (Postgres):** `localhost:5432`

> **Dica:** a interface web (porta 8080) encaminha qualquer chamada para `/api/...` ao mesmo serviÃ§o Flask na porta 5000. Exemplos: `http://localhost:5000/query` (direto) e `http://localhost:8080/api/query` (via UI).

---

## Endpoints da API

### Endpoint Principal do Agente

- **URL:** `POST /agent/ask`
- **Alias:** `POST /api/ask` (exposto pelo proxy da UI em `http://localhost:8080`)
- **DescriÃ§Ã£o:** Processa uma pergunta usando o fluxo completo do agente LangGraph. Suporta histÃ³rico de conversa e aceita overrides via payload.
- **Payload (JSON):**
  ```json
  {
    "question": "Qual Ã© o e-mail do departamento de biologia?",
    "messages": [
      {"role": "user", "content": "Qual o contato do depto de bio?"},
      {"role": "assistant", "content": "NÃ£o encontrei um departamento com esse nome. Poderia especificar o nome completo?"}
    ],
    "max_refine_attempts": 1
  }
  ```
  - `max_refine_attempts` (opcional) limita quantas reformulaÃ§Ãµes automÃ¡ticas o agente pode fazer na chamada (padrÃ£o: `AGENT_REFINE_MAX_ATTEMPTS`).
  - O corpo da resposta inclui `meta.query_hash`, `meta.refine_history`, `meta.refine_prompt_hashes` e `meta.confidence`, Ãºteis para correlacionar com os registros e mÃ©tricas.

### Endpoint Legado (RAG Direto)

- **URL:** `POST /query`
- **Alias:** `POST /api/query` (quando acessado via Nginx/porta 8080)
- **DescriÃ§Ã£o:** Processa uma pergunta usando apenas o pipeline de RAG direto, sem a camada do agente.

Ambos os endpoints retornam o campo opcional `"needs_clarification"` quando o sistema encontra mÃºltiplos candidatos (ex.: pessoas com o mesmo nome). Isso gera uma resposta enumerando as opÃ§Ãµes disponÃ­veis e incrementa o contador `queries_ambiguous` em `/metrics`.

---

## Cache de Respostas (Redis)

- O `docker-compose.*` agora sobe um serviÃ§o Redis (`ai_redis`) que atende ao cache de respostas.
- Configure `REDIS_URL` (padrÃ£o `redis://ai_redis:6379/0`) e `CACHE_TTL_SECONDS` (12 horas) para controlar a camada de cache.
- O ETL invalida o cache automaticamente ao final de rebuilds e atualizaÃ§Ãµes incrementais, e a variÃ¡vel `INDEX_VERSION` faz parte da assinatura das chaves.
- `/metrics` passou a expor `cache_hits_total` e `cache_misses_total`, permitindo acompanhar a eficiÃªncia do cache.
- Para garantir o funcionamento, hÃ¡ testes dedicados: `python -m pytest tests/test_api_cache.py`.
- Consulte `docs/METRICS.md` para exemplos de consulta e alertas Prometheus/Grafana.

---

## Auto-refine do Agente LangGraph

- Quando o RAG entrega baixa confianÃ§a, o agente tenta atÃ© duas reformulaÃ§Ãµes automÃ¡ticas antes de pedir mais contexto ao usuÃ¡rio.
- As novas consultas sÃ£o geradas via LLM (`refine_query_prompt.txt`) e reutilizam o pipeline completo de RAG, registrando o histÃ³rico (e hashes) em `meta.refine_history`/`meta.refine_prompt_hashes`.
- Novas mÃ©tricas expostas em `/metrics`: `agent_refine_attempts_total`, `agent_refine_success_total`, `agent_refine_exhausted_total` e `agent_low_confidence_total`.
- Ajuste a estratÃ©gia com `AGENT_REFINE_ENABLED`, `AGENT_REFINE_MAX_ATTEMPTS`, `AGENT_REFINE_CONFIDENCE`, o limiar de confianÃ§a por rota (`CONFIDENCE_MIN_AGENT`) e, via payload, `max_refine_attempts`.
- A suÃ­te `python -m pytest tests/test_agent_workflow.py` valida fluxos de sucesso e fallback.

## AvaliaÃ§Ã£o do Sistema

O projeto inclui um script de avaliaÃ§Ã£o de ponta a ponta que usa `ragas` e `langchain-google-genai` para calcular mÃ©tricas automÃ¡ticas.

1.  Garanta que as dependÃªncias estejam instaladas (`pip install -r requirements-cpu.txt`) e que a variÃ¡vel `GOOGLE_API_KEY` esteja configurada no ambiente.
2.  Inicie a API localmente (Docker Compose ou ambiente manual).
3.  Execute o script `eval_rag.py`, informando o dataset via argumento posicional ou pelo flag `--dataset`.

```bash
# Exemplo de execuÃ§Ã£o com saÃ­da em reports/
python eval_rag.py --dataset tests/eval_sample.csv --out reports/
```

Use `--agent-endpoint` e `--legacy-endpoint` para apontar para URLs especÃ­ficas quando necessÃ¡rio.

O relatÃ³rio consolida mÃ©tricas de **RecuperaÃ§Ã£o** (Recall, MRR, nDCG). Quando a chave `GOOGLE_API_KEY` estÃ¡ definida, o script tambÃ©m produz as mÃ©tricas de **GeraÃ§Ã£o** (Faithfulness e Answer Relevancy) via RAGAs.
> **Checklist rÃ¡pido:** o script agora emite mensagens explÃ­citas quando as dependÃªncias do RAGAs ou a `GOOGLE_API_KEY` estÃ£o ausentes. O teste `python -m pytest tests/test_eval_rag.py` cobre esses cenÃ¡rios e garante proteÃ§Ã£o regressiva.

### Dump de debug por pergunta

Para auditar rapidamente quais rotas, candidatos e metadados foram usados em cada pergunta do dataset, use:

```bash
python scripts/dump_eval_debug.py \
  --dataset evaluation_dataset.jsonl \
  --mode agent \
  --output reports/debug-dump-$(date +%Y%m%d-%H%M%S)
```

O script envia cada pergunta com `debug=true` e salva um JSON por item (incluindo `debug.timing_ms`, rota escolhida, top-k prÃ©/pÃ³s-rerank, contatos extraÃ­dos e confianÃ§a final). Em ambientes Windows/PowerShell substitua a variÃ¡vel de data por um nome fixo de pasta, por exemplo `--output reports/debug-dump`.

## Executando os Testes

O projeto utiliza `pytest` para testes automatizados. Para executar a suÃ­te de testes:

1.  Instale as dependÃªncias correspondentes ao seu ambiente (fora do Docker elas precisam incluir FAISS e Torch):
    ```bash
    pip install -r requirements-cpu.txt  # ou requirements-gpu.txt se estiver com CUDA
    ```
2.  Execute o pytest na raiz do projeto:
    ```bash
    pytest -v
    ```

### Checklist de validaÃ§Ã£o local
Siga os passos abaixo antes de publicar novas imagens ou documentaÃ§Ã£o (exemplo em PowerShell/Windows):

```bash
# Ambiente e dependÃªncias
python -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt

# AvaliaÃ§Ã£o RAGAs (gera relatÃ³rio em reports/)
set GOOGLE_API_KEY=SEU_TOKEN
python eval_rag.py ^
  --dataset evaluation_dataset.jsonl ^
  --agent-endpoint http://localhost:5000/agent/ask ^
  --legacy-endpoint http://localhost:5000/query ^
  --out reports/

# Smoke tests (API direta e via proxy)
python scripts/smoke_api.py http://localhost:5000/api
python scripts/smoke_api.py http://localhost:8080/api

# CenÃ¡rio funcional (telefone / contato)
curl -fsS http://localhost:8080/agent/ask ^
  -H "Content-Type: application/json" ^
  -d "{\"question\":\"Qual o telefone da Andreia da computacao?\",\"messages\":[],\"debug\":true}" ^
  | python -m json.tool
```
> A resposta deve vir pela rota `contact_fallback`, exibindo telefone e e-mail especÃ­ficos.

---

## Estrutura do Projeto

```
. C:/Temp/Workspace/rag-microservice
â”œâ”€â”€ ğŸ“„ .env.example        # Exemplo de arquivo de configuraÃ§Ã£o
â”œâ”€â”€ ğŸ“„ api.py              # Servidor Flask, expÃµe os endpoints da API
â”œâ”€â”€ ğŸ“„ agent_workflow.py   # Orquestra a lÃ³gica do agente com LangGraph
â”œâ”€â”€ ğŸ“„ query_handler.py    # Implementa a lÃ³gica central de RAG (busca e geraÃ§Ã£o)
â”œâ”€â”€ ğŸ“„ llm_client.py       # Cliente unificado e robusto para interagir com LLMs
â”œâ”€â”€ ğŸ“„ etl_orchestrator.py # Pipeline de ETL para construir o Ã­ndice vetorial
â”œâ”€â”€ ğŸ“„ eval_rag.py         # Script para avaliaÃ§Ã£o de ponta-a-ponta do sistema
â”œâ”€â”€ ğŸ“„ telemetry.py        # MÃ³dulo de logging de telemetria
â”œâ”€â”€ ğŸ“ loaders/           # MÃ³dulo unificado para carregar documentos de diferentes formatos
â”œâ”€â”€ ğŸ“ prompts/           # Armazena os prompts usados pelo agente e pelo RAG
â”œâ”€â”€ ğŸ“ data/              # ContÃ©m os documentos fonte para o ETL
â”œâ”€â”€ ğŸ“ config/            # Arquivos de configuraÃ§Ã£o adicionais (ex: ontologias)
â”œâ”€â”€ ğŸ“ tests/             # Testes automatizados com pytest
â”œâ”€â”€ ğŸ³ Dockerfile.cpu      # Define a imagem Docker para ambiente CPU
â”œâ”€â”€ ğŸ³ Dockerfile.gpu      # Define a imagem Docker para ambiente GPU
â”œâ”€â”€ ğŸ³ docker-compose.cpu.yml  # Stack completa otimizada para CPU
â””â”€â”€ ğŸ³ docker-compose.gpu.yml  # Stack completa com suporte a GPU
```

---

## VariÃ¡veis de Ambiente

As principais variÃ¡veis de ambiente para configurar o comportamento do sistema estÃ£o no arquivo `.env`. Consulte o `.env.example` para uma lista completa e descriÃ§Ãµes detalhadas. Destaques:

- **LLM**: `GOOGLE_API_KEY` e `GOOGLE_MODEL` (ex.: `models/gemini-2.5-flash-lite`).
- **Embeddings/FAISS**: `EMBEDDINGS_MODEL` (ETL e API precisam usar o mesmo valor) e `FAISS_STORE_DIR`.
- **Reranker**: `RERANKER_PRESET` (`off | fast | balanced | full`) e `RERANKER_ENABLED=true`. No `.env` recomendamos o preset `full` com o cross-encoder `cross-encoder/ms-marco-MiniLM-L-6-v2`, `RERANKER_CANDIDATES=48`, `RERANKER_TOP_K=10` e `RERANKER_MAX_LEN=512`. Ajuste conforme a latÃªncia aceitÃ¡vel e o hardware disponÃ­vel.
- **RecuperaÃ§Ã£o (BM25 + FAISS)**: `HYBRID_ENABLED` (default `true`), `RETRIEVAL_FETCH_K` (`80`), `RETRIEVAL_K` (`8`), `RETRIEVAL_MMR_LAMBDA` (`0.35`), `RETRIEVAL_MMR_LAMBDA_SHORT` (`0.25`), `RETRIEVAL_MIN_SCORE` (`0.25`), alÃ©m dos parÃ¢metros `MAX_PER_SOURCE`, `LEXICAL_THRESHOLD` e `DEPT_BONUS` para controlar boosts e diversidade.
- **Multi-query e confianÃ§a**: `MQ_ENABLED`, `MQ_VARIANTS`, `MQ_USE_LLM`, `MQ_LLM_MAX_VARIANTS`, `CONFIDENCE_MIN` (com overrides opcionais `CONFIDENCE_MIN_QUERY`/`CONFIDENCE_MIN_AGENT`) e `REQUIRE_CONTEXT`. Quando a confianÃ§a final cai abaixo de `RETRIEVAL_MIN_SCORE`, o sistema dispara automaticamente uma segunda rodada de busca antes de abrir o fluxo de esclarecimento ao usuÃ¡rio.
- **Formato da resposta**: `STRUCTURED_ANSWER` (markdown com resumo/fontes) e `MAX_SOURCES`.
- **Agente (auto-refine)**: `AGENT_REFINE_ENABLED`, `AGENT_REFINE_MAX_ATTEMPTS`, `AGENT_REFINE_CONFIDENCE`.
- **Ontologia/boosts institucionais**: `TERMS_YAML` aponta para `config/ontology/terms.yml` (ou caminho customizado). O mesmo valor deve ser usado na API e no ETL para manter sincronia de boosts, sinÃ´nimos e expansÃµes multi-query.

Caso queira forÃ§ar uma rota especÃ­fica para debug, use `ROUTE_FORCE=lexical|vector`.

---

## Status Atual e PrÃ³ximos Passos

### Panorama atual

- O pipeline de recuperaÃ§Ã£o combina BM25 + FAISS, aplica MMR (`Î»â‰ˆ0,3`) antes da fusÃ£o RRF e finaliza com o reranker `cross-encoder/ms-marco-MiniLM-L-6-v2`, entregando mais diversidade e precisÃ£o nos trechos recuperados.
- Logs e telemetria agora registram as listas top-30/top-8 antes e depois do rerank, alÃ©m da query efetivamente usada (incluindo a variaÃ§Ã£o automÃ¡tica quando `RETRIEVAL_MIN_SCORE` Ã© violado).
- O cache Redis, o auto-refine do agente e os testes de API continuam verdes (`pytest tests/test_api.py`), garantindo que as alteraÃ§Ãµes sejam regressÃ£o-free.

### AÃ§Ãµes sugeridas

1. **Rodar avaliaÃ§Ã£o objetiva**  
   Execute `python eval_rag.py --out reports/` (com `GOOGLE_API_KEY` configurada) para medir o impacto das mudanÃ§as nas mÃ©tricas RAGAs (`faithfulness`, `context_precision`, etc.) e versionar o relatÃ³rio gerado.

2. **Expandir ontologia e boosts**  
   Revise `config/ontology/terms.yml` para adicionar sinÃ´nimos institucionais, ajustar `boosts.department` e enriquecer `mq_expansions`. Isso reforÃ§a o BM25 e beneficia a rodada automÃ¡tica de retry.

3. **Monitorar as novas mÃ©tricas**  
   Acompanhe `/metrics` (novos contadores de cache, agent refine e low confidence) e os registros JSON em `logs/queries.log` para validar se o retry automÃ¡tico estÃ¡ ajudando â€” especialmente nas consultas de baixa confianÃ§a antes problemÃ¡ticas.

Para um roteiro mais detalhado de melhorias planejadas, consulte o documento [melhorias.md](docs/melhorias.md).



## DocumentaÃ§Ã£o adicional

Toda a documentaÃ§Ã£o complementar foi movida para a pasta docs/:

- [VisÃ£o de arquitetura](docs/ARCHITECTURE.md)
- [Contexto operacional](docs/contexto.md)
- [Plano de melhorias e backlog](docs/melhorias.md)
- [Telemetria e observabilidade](docs/TELEMETRY.md)
- [PublicaÃ§Ã£o de imagens Docker](docs/publish_images.md)

### Imagens prÃ©-construÃ­das (opcional)

Resumo rÃ¡pido:
1. FaÃ§a login no registry (ex.: docker login, docker login ghcr.io).
2. Rode ./scripts/build_and_publish_images.sh --prefix SEU_PREFIXO --tag SUA_TAG --push.
3. Configure as variÃ¡veis RAG_IMAGE_PREFIX e RAG_IMAGE_TAG (via shell ou arquivo .env).
4. Depois execute docker-compose pull + docker-compose up para usar as imagens publicadas.

Para o passo a passo completo consulte [docs/publish_images.md](docs/publish_images.md).
ObservaÃ§Ã£o: o script gera imagens para ai_projeto_api e ai_etl. A UI continua usando nginx:1.27-alpine e nÃ£o precisa de push.
